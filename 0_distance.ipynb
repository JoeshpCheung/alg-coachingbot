{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d3f674ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# simhash,推荐使用\n",
    "class SimhashStr():\n",
    "    def __init__(self, tfidf_dict=None):\n",
    "        self.tfidf_dict = tfidf_dict\n",
    "\n",
    "    # 得到输入字符串的hash值\n",
    "    def get_hash(self, text, topK=None):\n",
    "        # 结巴分词\n",
    "        seg = jieba.cut(text)\n",
    "        # print(len(set(list(jieba.cut(text)))))\n",
    "        # 取前20个关键词\n",
    "        # 这个可以替换成我们的tfidf词表\n",
    "        if self.tfidf_dict is None:\n",
    "            keyword = jieba.analyse.extract_tags('|'.join(seg), topK=topK, withWeight=True, allowPOS=())\n",
    "            # 若jieba解析不出任何词汇，则默认使用所有词汇，并赋权1\n",
    "            if not keyword:\n",
    "                keyword = [(i, 1) for i in list(jieba.cut(text))]\n",
    "        else:\n",
    "            # 若传入了自定义tfidf词典，则用自己的\n",
    "            keyword = dict()\n",
    "            for i in list(seg):\n",
    "                if i in d:\n",
    "                    if i in keyword:\n",
    "                        keyword[i] += d[i]\n",
    "                    else:\n",
    "                        keyword[i] = d[i]\n",
    "            keyword = sorted(keyword.items(),key = lambda x:x[1],reverse = True)[:topK]\n",
    "            \n",
    "        keyList = []\n",
    "        # 获取每个词的权重\n",
    "        for feature, weight in keyword:\n",
    "            # 每个关键词的权重*总单词数\n",
    "            weight = int(weight * 20)\n",
    "            # 获取每个关键词的特征\n",
    "            feature = self.string_hash(feature)\n",
    "            temp = []\n",
    "            # 获取每个关键词的权重\n",
    "            for i in feature:\n",
    "                if i == '1':\n",
    "                    temp.append(weight)\n",
    "                else:\n",
    "                    temp.append(-weight)\n",
    "                keyList.append(temp)\n",
    "        # 将每个关键词的权重变成一维矩阵\n",
    "        list1 = np.sum(np.array(keyList), axis=0)\n",
    "        # 获取simhash值\n",
    "        simhash = ''\n",
    "        for i in list1:\n",
    "            # 对特征标准化表示\n",
    "            if i > 0:\n",
    "                simhash = simhash + '1'\n",
    "            else:\n",
    "                simhash = simhash + '0'\n",
    "        return simhash\n",
    "\n",
    "    def string_hash(self, feature):\n",
    "        if feature == \"\":\n",
    "            return 0\n",
    "        else:\n",
    "            # 将字符转为二进制，并向左移动7位\n",
    "            x = ord(feature[0]) << 7\n",
    "            m = 1000003\n",
    "            mask = 2 ** 128 - 1\n",
    "            # 拼接每个关键词中字符的特征\n",
    "            for c in feature:\n",
    "                x = ((x * m) ^ ord(c)) & mask\n",
    "            x ^= len(feature)\n",
    "            if x == -1:\n",
    "                x = -2\n",
    "            # 获取关键词的64位表示\n",
    "            x = bin(x).replace('0b', '').zfill(64)[-64:]\n",
    "            return str(x)\n",
    "        \n",
    "    def get_distance(self, sim1, sim2):\n",
    "        # 转为二进制结构\n",
    "        t1 = '0b' + sim1\n",
    "        t2 = '0b' + sim2\n",
    "        \n",
    "        max_hashbit = max(len(t1), len(t2))\n",
    "        \n",
    "        n = int(t1, 2) ^ int(t2, 2)\n",
    "        # 相当于对每一位进行异或操作\n",
    "        i = 0\n",
    "        while n:\n",
    "            n &= (n - 1)\n",
    "            i += 1\n",
    "        return i, max_hashbit\n",
    "    \n",
    "    def similar(self, text1, text2, topK=20):\n",
    "        simhash1 = self.get_hash(text1, topK=topK)\n",
    "        simhash2 = self.get_hash(text2, topK=topK)\n",
    "\n",
    "        # 汉明距离\n",
    "        distince, max_hashbit = self.get_distance(simhash1, simhash2)\n",
    "        similar = 1 - distince / max_hashbit\n",
    "        return similar\n",
    "    \n",
    "\n",
    "# 编辑距离相似度\n",
    "def editdistance(str1, str2, type_='score'):\n",
    "    # cal Levenshtein Distance(Edit Distance)\n",
    "    edit = [[i+j for j in range(len(str2)+1)] for i in range(len(str1)+1)]\n",
    "    for i in range(1, len(str1)+1):\n",
    "        for j in range(1, len(str2)+1):\n",
    "            if str1[i-1] == str2[j-1]:\n",
    "                d = 0\n",
    "            else:\n",
    "                d = 1\n",
    "                \n",
    "            edit[i][j] = min(edit[i-1][j]+1, edit[i][j-1]+1, edit[i-1][j-1]+d)\n",
    "    res = edit[len(str1)][len(str2)]\n",
    "    if type_ == 'score':\n",
    "        res = 1 - res/max(len(str1), len(str2))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f4c56cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len:  83 83\n",
      "Simhash similar:  0.7727272727272727\n",
      "editdistance similar:  0.9156626506024097\n",
      "Simhash similar:  0.4696969696969697\n",
      "editdistance similar:  0.8\n"
     ]
    }
   ],
   "source": [
    "text1 = '中国国务院扶贫办最近表示，中国脱贫攻坚已经取得决定性成就，到今年年底，所有贫困人口会全部退出。作为其践行社会责任的重要一环，汇丰中国一直致力推动大众就业、支持全面脱贫'\n",
    "text2 = '美国国安局扶贫办最近表示，美国脱贫攻坚已经取得决定性成就，到今年年底，所有贫困人口会全部退出。作为其践行社会责任的重要一环，花旗美国一直致力推动大众就业、支持全面脱贫'\n",
    "print('len: ', len(text1), len(text2))\n",
    "res = SimhashStr().similar(text1, text2)\n",
    "print('Simhash similar: ', res)\n",
    "res = editdistance(text1, text2)\n",
    "print('editdistance similar: ', res)\n",
    "\n",
    "text1 = '今天心情好'\n",
    "text2 = '今天心情坏'\n",
    "res = SimhashStr().similar(text1, text2)\n",
    "print('Simhash similar: ', res)\n",
    "res = editdistance(text1, text2)\n",
    "print('editdistance similar: ', res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8ed83198",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/home/jasoncheung/project/work/alg-coachingbot/datas/'\n",
    "df_normal = pd.read_excel(dir_path+'normal_QC.xlsx')\n",
    "df_bank = pd.read_excel(dir_path+'bank_QC.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "7b20ebb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank EditDistance MSE:  0.3121040805733899\n",
      "bank SimHash MSE:  0.25456280156941313\n",
      "normal EditDistance MSE:  0.04814843628120935\n",
      "normal SimHash MSE:  0.24849385616796132\n"
     ]
    }
   ],
   "source": [
    "# cal simhash similar & editdistance similar\n",
    "normal_score_ed = []\n",
    "for t1, t2 in zip(df_normal.text1.tolist(), df_normal.text2.tolist()):\n",
    "    tmp_score = editdistance(t1, t2)\n",
    "    normal_score_ed.append(tmp_score)\n",
    "df_normal['ed_score'] = normal_score_ed\n",
    "\n",
    "normal_score_sh = []\n",
    "for t1, t2 in zip(df_normal.text1.tolist(), df_normal.text2.tolist()):\n",
    "    tmp_score = SimhashStr().similar(t1, t2)\n",
    "    normal_score_sh.append(tmp_score)\n",
    "df_normal['simhash_score'] = normal_score_sh\n",
    "\n",
    "\n",
    "bank_score_ed = []\n",
    "for t1, t2 in zip(df_bank.text1.tolist(), df_bank.text2.tolist()):\n",
    "    tmp_score = editdistance(t1, t2)\n",
    "    bank_score_ed.append(tmp_score)\n",
    "df_bank['ed_score'] = bank_score_ed\n",
    "\n",
    "bank_score_sh = []\n",
    "for t1, t2 in zip(df_bank.text1.tolist(), df_bank.text2.tolist()):\n",
    "    tmp_score = SimhashStr().similar(t1, t2)\n",
    "    bank_score_sh.append(tmp_score)\n",
    "df_bank['simhash_score'] = bank_score_sh\n",
    "\n",
    "# calculate MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "res_bank_ed = mean_squared_error(df_bank.score.tolist(), df_bank.ed_score.tolist())\n",
    "res_bank_sh = mean_squared_error(df_bank.score.tolist(), df_bank.simhash_score.tolist())\n",
    "\n",
    "res_normal_ed = mean_squared_error(df_normal.score.tolist(), df_normal.ed_score.tolist())\n",
    "res_normal_sh = mean_squared_error(df_normal.score.tolist(), df_normal.simhash_score.tolist())\n",
    "print('bank EditDistance MSE: ', res_bank_ed)\n",
    "print('bank SimHash MSE: ', res_bank_sh)\n",
    "\n",
    "print('normal EditDistance MSE: ', res_normal_ed)\n",
    "print('normal SimHash MSE: ', res_normal_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfef56a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal.to_excel(dir_path+'normal_QC.xlsx', index=False)\n",
    "df_bank.to_excel(dir_path+'bank_QC.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
